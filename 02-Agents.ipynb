{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1571b632",
      "metadata": {
        "id": "1571b632"
      },
      "source": [
        "# Agents ðŸ¤–\n",
        "\n",
        "Agents are like \"tools\" for LLMs. They allow a LLM to access Google search, perform complex calculations with Python, and even make SQL queries.\n",
        "\n",
        "In this notebook we'll explore agents and how to use them in LangChain.\n",
        "\n",
        "We'll start by installing the prerequisite libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZR3iGJJtdDE",
        "outputId": "0119ff4e-61fc-4d5f-9292-a719c54b8cb5"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain openai google-search-results wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02c4fa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02c4fa2",
        "outputId": "ad22dddc-6180-48ac-86dd-265b6b7afa57"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"xxx\"\n",
        "SERPAPI_KEY = 'xxx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bfcbb6",
      "metadata": {
        "id": "73bfcbb6"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "As we did before, we will be counting our tokens in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(agent, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = agent(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd60a97c",
      "metadata": {
        "id": "bd60a97c"
      },
      "source": [
        "With all of that set up, let's jump into **Agents**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What is an agent?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b919c3a",
      "metadata": {
        "id": "5b919c3a"
      },
      "source": [
        "**Definition**: The key behind agents is giving LLM's the possibility of using tools in their workflow. This is where langchain departs from the popular chatgpt implementation and we can start to get a glimpse of what it offers us as builders. Until now, we covered several building blocks in isolation. Let's see them come to life.\n",
        "\n",
        "The official definition of agents is the following:\n",
        "\n",
        "\n",
        "> Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c9c13e9",
      "metadata": {
        "id": "6c9c13e9"
      },
      "source": [
        "In this edition we will cover what we may call 'generic' agents which really able to perform many meta tasks. There are other more specific agents that are tuned for different tasks (called 'agent-toolkits'), but we will cover those in a future edition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6325f758",
      "metadata": {
        "id": "6325f758"
      },
      "source": [
        "## Agent types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d732b7a",
      "metadata": {
        "id": "4d732b7a"
      },
      "source": [
        "In this section we will review several agents and see how they 'think' and what they can do."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee98631",
      "metadata": {
        "id": "8ee98631"
      },
      "source": [
        "Using one of langchain's pre-built agents involves three variables:\n",
        "* defining the tools or the toolkit\n",
        "* defining the llm\n",
        "* defining the agent type\n",
        "\n",
        "This is all really easy to do in langchain, as we will see in the following example."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d70642",
      "metadata": {
        "id": "04d70642"
      },
      "source": [
        "### Agent type #1: Conversational React"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec589b1",
      "metadata": {
        "id": "1ec589b1"
      },
      "source": [
        "The zero shot agent is really interesting but, as we said before, it has no memory. What if we want an assistant that remembers things we have talked about and can also reason about them and use tools? For that we have the conversational react agent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c26c50f",
      "metadata": {
        "id": "5c26c50f"
      },
      "source": [
        "We will use the math tool in this example and load it as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6faff3",
      "metadata": {
        "id": "4b6faff3"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "tools = load_tools(\n",
        "    [\"llm-math\"],\n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be45b903",
      "metadata": {
        "id": "be45b903"
      },
      "source": [
        "The memory type being used here is a simple buffer memory to allow us to remember previous steps in the reasoning chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aff4edf",
      "metadata": {
        "id": "0aff4edf"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6579cef0",
      "metadata": {
        "id": "6579cef0"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "conversational_agent = initialize_agent(\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cabbea50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cabbea50",
        "outputId": "760bde78-a553-46e0-da3b-433cd0196613"
      },
      "outputs": [],
      "source": [
        "result = count_tokens(\n",
        "    conversational_agent,\n",
        "    \"What's the result of an investment of $10,000 growing at 8% annually for 5 years with compound interest?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdcd9847",
      "metadata": {
        "id": "bdcd9847"
      },
      "source": [
        "As we can see below, the prompt is similar but it includes a great prelude of instructions that make it an effective assistant as well + a spot for including the chat history from the memory component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de413fd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de413fd3",
        "outputId": "f79dc083-db80-4dbb-8fb6-da5f6f603645"
      },
      "outputs": [],
      "source": [
        "print(conversational_agent.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ee8041",
      "metadata": {
        "id": "32ee8041"
      },
      "source": [
        "Let's see what happens if we try to answer the question that is related to the previous one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e109878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e109878",
        "outputId": "51a80167-1e1b-4717-e3bc-bf25ad1c6cb7"
      },
      "outputs": [],
      "source": [
        "result = count_tokens(\n",
        "    conversational_agent,\n",
        "    \"If we start with $15,000 instead and follow the same 8% annual growth for 5 years with compound interest, how much more would we have compared to the previous scenario?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44135b8d",
      "metadata": {
        "id": "44135b8d"
      },
      "source": [
        "### Agent type #2: React Docstore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e6d0d13",
      "metadata": {
        "id": "1e6d0d13"
      },
      "source": [
        "This type of agent is similar to the ones we have seen so far but it includes the interaction with a docstore. It will have two and only two tools at its disposal: 'Search' and 'Lookup'."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d569fd41",
      "metadata": {
        "id": "d569fd41"
      },
      "source": [
        "With 'Search' it will bring up a relevant article and with 'Lookup' the agent will find the right piece of information in the article. This is probably easiest to see in an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc452af",
      "metadata": {
        "id": "ecc452af"
      },
      "outputs": [],
      "source": [
        "from langchain import Wikipedia\n",
        "from langchain.agents.react.base import DocstoreExplorer, Tool\n",
        "\n",
        "docstore=DocstoreExplorer(Wikipedia())\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=docstore.search,\n",
        "        description='search wikipedia'\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Lookup\",\n",
        "        func=docstore.lookup,\n",
        "        description='lookup a term in wikipedia'\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595938a1",
      "metadata": {
        "id": "595938a1"
      },
      "outputs": [],
      "source": [
        "docstore_agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=\"react-docstore\",\n",
        "    verbose=True,\n",
        "    max_iterations=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba6b065",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba6b065",
        "outputId": "52d4f8f6-b56b-4894-9f18-dc126be884b4"
      },
      "outputs": [],
      "source": [
        "count_tokens(docstore_agent, \"How high is the Stuttgart Fernsehturm in meters?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5f77ac",
      "metadata": {
        "id": "6a5f77ac"
      },
      "source": [
        "The prompt to this agent contains several examples of the `Question` > `Thought` > `Action` > `Observation` loop, that include the `Search` and `Lookup` tools.\n",
        "\n",
        "If you want to learn more about this approach [this](https://arxiv.org/pdf/2210.03629.pdf) is the paper for ReAct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc8f3bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(docstore_agent.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aae3b5b",
      "metadata": {
        "id": "4aae3b5b"
      },
      "source": [
        "### Agent type #3: Self Ask with Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c07039e5",
      "metadata": {
        "id": "c07039e5"
      },
      "source": [
        "This is the first-choice agent to use when using LLM's to extract information with a search engine. The agent will ask follow-up questions and use the search functionality to get intermediate answers that help it get to a final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903660b2",
      "metadata": {
        "id": "903660b2"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI, SerpAPIWrapper\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "\n",
        "search = SerpAPIWrapper(serpapi_api_key=SERPAPI_KEY)\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Intermediate Answer\",\n",
        "        func=search.run,\n",
        "        description='google search'\n",
        "    )\n",
        "]\n",
        "\n",
        "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec1c4c8",
      "metadata": {
        "id": "0ec1c4c8"
      },
      "source": [
        "We will not interact with this agent because for that we would need a serpapi key. However, by checking out the prompt we can see a few examples of how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4134930",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4134930",
        "outputId": "2f58884d-0c4e-4b40-de45-6928e0c60ef4"
      },
      "outputs": [],
      "source": [
        "count_tokens(self_ask_with_search, \"What is the profession of Prof. Markus Reischl?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e7790ae",
      "metadata": {
        "id": "3e7790ae"
      },
      "source": [
        "As we can see, the prompt is basically a series of many examples to show the LLM how to ask follow up questions to a search tool until it can get to the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3ec730",
      "metadata": {},
      "source": [
        "# Outlook\n",
        "This is just a very quick introduction into using LLMs with LangChain and the OpenAI API. We merely scratched the surface of whats possible. If you want to continue learning about the practical applications of LLMs i highly recommend the following ressoucres:\n",
        "\n",
        "1. The Youtube series [LangChain for Gen AI and LLMs](https://www.youtube.com/watch?v=nE2skSRWTTs&list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F) of James Briggs.\n",
        "2. The [LangChain Handbook Code](https://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/handbook\n",
        ") (of which this tutorial is based on).\n",
        "3. The [LangChain documentation](https://python.langchain.com/docs/get_started/introduction) (duh!)\n",
        "4. The [LangChain Hub](https://smith.langchain.com/o/dddd8041-57d1-56c4-a1c9-aa67b55323c1) containing many prompts, chains, agents, and more (still in closed beta)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "578e1e8dce4dc6c542f1ea2d66a2d9db6ef592936dcc314004bdae386f827d38"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
